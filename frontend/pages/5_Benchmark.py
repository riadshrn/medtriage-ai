"""
Benchmark √âco-Performance - Comparaison des mod√®les Mistral.

Compare les mod√®les Mistral sur les 3 usages principaux afin de trouver
le mod√®le le plus sobre √©nerg√©tiquement tout en restant performant.
"""

import os
import sys
import json
from pathlib import Path
from typing import Dict, List, Optional

import requests
import streamlit as st
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Configuration des chemins pour importer depuis le parent
current_dir = Path(__file__).parent.parent
sys.path.insert(0, str(current_dir))

from state import init_session_state
from style import configure_page, apply_style

# Initialisation
configure_page(page_title="Benchmark - MedTriage-AI", page_icon="üå±")
init_session_state()
apply_style()

# Configuration API
API_URL = os.getenv("API_URL", "http://backend:8000")

# Configuration des mod√®les (4 tailles) - Prix officiels Mistral AI 2025
# Source : https://mistral.ai/pricing
MODELES_MISTRAL = {
    "ministral-3b-latest": {
        "display_name": "Ministral 3B",
        "input_price": 0.04,
        "output_price": 0.04,
        "description": "Ultra-l√©ger",
        "icon": "ü™∂"
    },
    "mistral-small-latest": {
        "display_name": "Mistral Small",
        "input_price": 0.1,
        "output_price": 0.3,
        "description": "Rapide",
        "icon": "üöÄ"
    },
    "mistral-medium-latest": {
        "display_name": "Mistral Medium",
        "input_price": 0.4,
        "output_price": 2.0,
        "description": "√âquilibr√©",
        "icon": "‚öñÔ∏è"
    },
    "mistral-large-latest": {
        "display_name": "Mistral Large",
        "input_price": 2.0,
        "output_price": 6.0,
        "description": "Pr√©cision",
        "icon": "üéØ"
    }
}

# Cas de simulation pr√©d√©finis avec plusieurs questions
SIMULATION_CASES = {
    "douleur_thoracique": {
        "name": "Douleur Thoracique",
        "persona": """Tu es un patient de 58 ans, homme, fumeur (30 ans, 1 paquet/jour).
SYMPT√îMES : Douleur thoracique intense irradiant vers le bras gauche depuis 1h. Tu transpires et as des naus√©es.
ANT√âC√âDENTS : Hypertension, cholest√©rol. P√®re d√©c√©d√© d'infarctus √† 55 ans.
CONSTANTES : FC=110, PA=155/95, T=37.2, SpO2=94%, Douleur=8/10
COMPORTEMENT : Tu es tr√®s anxieux, tu as peur de mourir.""",
        "questions": [
            "Pouvez-vous me d√©crire votre douleur ?",
            "Depuis quand avez-vous mal ?",
            "Avez-vous des ant√©c√©dents cardiaques ?",
            "Prenez-vous des m√©dicaments ?"
        ]
    },
    "detresse_respiratoire": {
        "name": "D√©tresse Respiratoire",
        "persona": """Tu es une patiente de 45 ans, asthmatique connue.
SYMPT√îMES : Crise s√©v√®re depuis 30min, tu parles difficilement par phrases courtes.
ANT√âC√âDENTS : Asthme s√©v√®re, nombreuses hospitalisations. Tu n'as pas pris ton traitement depuis 2 jours.
CONSTANTES : FC=120, PA=140/90, T=37.0, SpO2=88%, FR=28, Douleur=6/10
COMPORTEMENT : Tu es paniqu√©e, tu n'arrives pas √† reprendre ton souffle.""",
        "questions": [
            "Depuis quand avez-vous du mal √† respirer ?",
            "Avez-vous pris votre traitement ?",
            "Est-ce que vous pouvez me dire une phrase compl√®te ?",
            "Avez-vous d√©j√† eu des crises comme √ßa ?"
        ]
    },
    "douleur_abdominale": {
        "name": "Douleur Abdominale",
        "persona": """Tu es une patiente de 35 ans.
SYMPT√îMES : Douleur abdominale intense en fosse iliaque droite depuis 12h. Fi√®vre et naus√©es depuis ce matin.
ANT√âC√âDENTS : Aucun ant√©c√©dent notable.
CONSTANTES : FC=95, PA=125/80, T=38.5, SpO2=98%, Douleur=7/10
COMPORTEMENT : Tu as mal quand on appuie sur le ventre √† droite et tu n'as pas pu manger.""",
        "questions": [
            "O√π avez-vous mal exactement ?",
            "Depuis quand avez-vous cette douleur ?",
            "Avez-vous de la fi√®vre ?",
            "Avez-vous pu manger ?"
        ]
    },
    "traumatisme_cranien": {
        "name": "Traumatisme Cr√¢nien",
        "persona": """Tu es un patient de 25 ans, chute de v√©lo il y a 30min.
SYMPT√îMES : Mal √† la t√™te, un peu confus, bosse sur le front. Tu ne te souviens pas bien de l'accident.
ANT√âC√âDENTS : Aucun.
CONSTANTES : FC=85, PA=130/85, T=36.8, SpO2=99%, Glasgow=14, Douleur=5/10
COMPORTEMENT : Tu as des naus√©es et la lumi√®re te g√™ne.""",
        "questions": [
            "Vous souvenez-vous de ce qui s'est pass√© ?",
            "Avez-vous perdu connaissance ?",
            "Avez-vous des naus√©es ou des vomissements ?",
            "Est-ce que la lumi√®re vous g√™ne ?"
        ]
    }
}

# Labels √©nerg√©tiques
ENERGY_LABELS = {
    "A": {"color": "#22C55E", "description": "Excellent"},
    "B": {"color": "#84CC16", "description": "Tr√®s bien"},
    "C": {"color": "#EAB308", "description": "Correct"},
    "D": {"color": "#F97316", "description": "√âlev√©"},
    "E": {"color": "#EF4444", "description": "Tr√®s √©lev√©"}
}

# Couleurs des niveaux de triage
TRIAGE_COLORS = {
    "ROUGE": "#DC2626",
    "JAUNE": "#F59E0B",
    "VERT": "#10B981",
    "GRIS": "#6B7280"
}

# Ordre d'affichage des mod√®les (du plus petit au plus grand)
MODEL_ORDER = [
    "ministral-3b-latest",
    "mistral-small-latest",
    "mistral-medium-latest",
    "mistral-large-latest"
]


def sort_results_by_model_size(results: Dict) -> Dict:
    """Trie les r√©sultats par taille de mod√®le (Mini > Small > Medium > Large)."""
    sorted_results = {}
    for model_key in MODEL_ORDER:
        if model_key in results:
            sorted_results[model_key] = results[model_key]
    return sorted_results


def init_benchmark_state():
    """Initialise l'√©tat de la session."""
    defaults = {
        "benchmark_results": {},
        "selected_models_extraction": ["ministral-3b-latest", "mistral-small-latest"],
        "selected_models_agent": ["ministral-3b-latest", "mistral-small-latest"],
        "selected_models_simulation": ["ministral-3b-latest", "mistral-small-latest"],
        "conversations_cache": None
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value


def reset_benchmark_state():
    """Remet √† z√©ro l'√©tat du benchmark."""
    st.session_state.benchmark_results = {}
    st.session_state.selected_models_extraction = ["ministral-3b-latest", "mistral-small-latest"]
    st.session_state.selected_models_agent = ["ministral-3b-latest", "mistral-small-latest"]
    st.session_state.selected_models_simulation = ["ministral-3b-latest", "mistral-small-latest"]


def load_conversations() -> List[Dict]:
    """Charge la liste des conversations depuis l'API."""
    if st.session_state.conversations_cache:
        return st.session_state.conversations_cache
    try:
        response = requests.get(f"{API_URL}/conversation/list", timeout=10)
        if response.status_code == 200:
            st.session_state.conversations_cache = response.json()
            return st.session_state.conversations_cache
    except requests.RequestException:
        pass
    return []


def load_conversation_content(filename: str) -> Optional[Dict]:
    """Charge le contenu d'une conversation."""
    try:
        response = requests.get(f"{API_URL}/conversation/load/{filename}", timeout=10)
        if response.status_code == 200:
            return response.json()
    except requests.RequestException:
        pass
    return None


def calculate_energy_label(energy_wh: float, use_case: str) -> str:
    """Calcule le label √©nerg√©tique (A-E)."""
    thresholds = {
        "extraction": {"A": 0.5, "B": 1.0, "C": 2.0, "D": 5.0},
        "agent": {"A": 1.0, "B": 2.0, "C": 5.0, "D": 10.0},
        "simulation": {"A": 0.3, "B": 0.8, "C": 1.5, "D": 3.0}
    }
    t = thresholds.get(use_case, thresholds["extraction"])

    if energy_wh <= t["A"]:
        return "A"
    elif energy_wh <= t["B"]:
        return "B"
    elif energy_wh <= t["C"]:
        return "C"
    elif energy_wh <= t["D"]:
        return "D"
    return "E"


def call_benchmark_api(endpoint: str, payload: Dict) -> Optional[Dict]:
    """Appelle l'API de benchmark."""
    try:
        response = requests.post(f"{API_URL}/benchmark/{endpoint}", json=payload, timeout=120)
        if response.status_code == 200:
            return response.json()
    except requests.RequestException:
        pass
    return None


def process_result(result: Dict, model: str, use_case: str) -> Dict:
    """Traite le r√©sultat d'un benchmark."""
    if result and result.get("success"):
        metrics = result.get("metrics", {})
        energy_wh = (metrics.get("energy_kwh", 0) or 0) * 1000
        co2_g = (metrics.get("gwp_kgco2", 0) or 0) * 1000

        return {
            "model_key": model,
            "model_name": MODELES_MISTRAL[model]["display_name"],
            "icon": MODELES_MISTRAL[model]["icon"],
            "success": True,
            "latency_s": metrics.get("latency_s", 0) or 0,
            "cost_usd": metrics.get("cost_usd", 0) or 0,
            "co2_g": co2_g,
            "energy_wh": energy_wh,
            "energy_label": calculate_energy_label(energy_wh, use_case),
            "total_tokens": metrics.get("total_tokens", 0),
            "input_tokens": metrics.get("input_tokens", 0),
            "output_tokens": metrics.get("output_tokens", 0),
            "raw_response": result.get("response") or result.get("analysis") or result.get("extracted_data"),
            "google_equiv": co2_g / 0.2 if co2_g > 0 else 0,
            "bulb_minutes": energy_wh
        }
    return {
        "model_key": model,
        "model_name": MODELES_MISTRAL[model]["display_name"],
        "icon": MODELES_MISTRAL[model]["icon"],
        "success": False,
        "error": result.get("error", "Erreur inconnue") if result else "Pas de r√©ponse"
    }


def render_model_selector(key: str) -> List[str]:
    """Affiche le s√©lecteur de mod√®les avec des boutons toggle."""
    st.markdown("#### Mod√®les √† comparer")

    cols = st.columns(len(MODELES_MISTRAL))
    selected = st.session_state.get(f"selected_models_{key}", [])

    for idx, (model_key, model_info) in enumerate(MODELES_MISTRAL.items()):
        with cols[idx]:
            is_selected = model_key in selected

            if st.button(
                f"{model_info['icon']} {model_info['display_name']}",
                key=f"btn_{key}_{model_key}",
                type="primary" if is_selected else "secondary",
                use_container_width=True
            ):
                if is_selected:
                    selected.remove(model_key)
                else:
                    selected.append(model_key)
                st.session_state[f"selected_models_{key}"] = selected
                st.rerun()

            st.caption(f"{model_info['description']}")
            st.caption(f"${model_info['input_price']}/M in ¬∑ ${model_info['output_price']}/M out")

    return selected


def render_comparison_chart(results: Dict):
    """Affiche le graphique comparatif avec tooltips informatifs."""
    # Tri par taille de mod√®le : Mini > Small > Medium > Large
    valid = sort_results_by_model_size({k: v for k, v in results.items() if v.get("success")})
    if len(valid) < 2:
        return

    models = [v["model_name"] for v in valid.values()]
    colors = ["#10B981", "#3B82F6", "#F59E0B", "#EF4444"][:len(models)]

    # Donn√©es pour tooltips enrichis
    energy_vals = [v["energy_wh"] for v in valid.values()]
    co2_vals = [v["co2_g"] for v in valid.values()]
    cost_vals = [v["cost_usd"] for v in valid.values()]

    # Calcul des √©quivalences pour tooltips
    google_equivs = [v["google_equiv"] for v in valid.values()]
    bulb_mins = [v["bulb_minutes"] for v in valid.values()]

    fig = make_subplots(
        rows=1, cols=3,
        subplot_titles=("‚ö° √ânergie (Wh)", "üåø CO‚ÇÇ (g)", "üí∞ Co√ªt ($)")
    )

    # √ânergie avec tooltip enrichi
    fig.add_trace(go.Bar(
        x=models,
        y=energy_vals,
        marker_color=colors,
        showlegend=False,
        hovertemplate="<b>%{x}</b><br>" +
                      "√ânergie : %{y:.3f} Wh<br>" +
                      "‚âà %{customdata:.1f} min d'ampoule 60W<br>" +
                      "<extra></extra>",
        customdata=bulb_mins
    ), row=1, col=1)

    # CO2 avec tooltip enrichi
    fig.add_trace(go.Bar(
        x=models,
        y=co2_vals,
        marker_color=colors,
        showlegend=False,
        hovertemplate="<b>%{x}</b><br>" +
                      "CO‚ÇÇ : %{y:.4f} g<br>" +
                      "‚âà %{customdata:.1f} recherches Google<br>" +
                      "<extra></extra>",
        customdata=google_equivs
    ), row=1, col=2)

    # Co√ªt avec tooltip enrichi
    fig.add_trace(go.Bar(
        x=models,
        y=cost_vals,
        marker_color=colors,
        showlegend=False,
        hovertemplate="<b>%{x}</b><br>" +
                      "Co√ªt : $%{y:.5f}<br>" +
                      "<extra></extra>"
    ), row=1, col=3)

    fig.update_layout(
        height=300,
        margin=dict(t=40, b=20),
        hoverlabel=dict(
            bgcolor="white",
            font_size=12,
            font_family="system-ui"
        )
    )
    st.plotly_chart(fig, use_container_width=True)


def render_winner_badge(results: Dict):
    """Affiche le gagnant du benchmark."""
    valid = {k: v for k, v in results.items() if v.get("success")}
    if len(valid) < 2:
        return

    winner = min(valid.items(), key=lambda x: x[1]["energy_wh"])

    st.success(f"**{winner[1]['model_name']}** est le plus sobre avec "
               f"**{winner[1]['energy_wh']:.2f} Wh** et **{winner[1]['co2_g']:.3f} g CO‚ÇÇ**")


def run_extraction_benchmark(models: List[str], text: str, progress) -> Dict:
    """Ex√©cute le benchmark d'extraction."""
    results = {}
    for idx, model in enumerate(models):
        progress.progress((idx + 1) / len(models), f"Test de {MODELES_MISTRAL[model]['display_name']}...")
        result = call_benchmark_api("extraction", {"text": text, "model": model})
        results[model] = process_result(result, model, "extraction")
    return results


def run_agent_benchmark(models: List[str], conversation: Dict, progress) -> Dict:
    """Ex√©cute le benchmark de l'agent."""
    results = {}
    for idx, model in enumerate(models):
        progress.progress((idx + 1) / len(models), f"Test de {MODELES_MISTRAL[model]['display_name']}...")
        result = call_benchmark_api("agent", {"conversation": conversation, "model": model})
        results[model] = process_result(result, model, "agent")
    return results


def run_simulation_benchmark(models: List[str], case: Dict, nurse_message: str, progress) -> Dict:
    """Ex√©cute le benchmark de simulation."""
    results = {}
    for idx, model in enumerate(models):
        progress.progress((idx + 1) / len(models), f"Test de {MODELES_MISTRAL[model]['display_name']}...")
        payload = {"persona": case["persona"], "history": [], "nurse_message": nurse_message, "model": model}
        result = call_benchmark_api("simulation", payload)
        results[model] = process_result(result, model, "simulation")
    return results


def render_compact_metrics(data: Dict):
    """Affiche les m√©triques de fa√ßon compacte dans un container."""
    label_config = ENERGY_LABELS.get(data.get("energy_label", "E"))

    st.markdown(f"""
    <div style="display: flex; align-items: center; gap: 15px; margin-bottom: 10px;">
        <div style="
            width: 45px; height: 45px;
            background: {label_config['color']};
            border-radius: 8px;
            display: flex; align-items: center; justify-content: center;
            font-size: 1.3rem; font-weight: 800; color: white;
        ">{data.get('energy_label', '?')}</div>
        <div>
            <div style="font-weight: 600; font-size: 0.95rem;">{data['icon']} {data['model_name']}</div>
            <div style="font-size: 0.75rem; color: #666;">
                ‚ö° {data['energy_wh']:.3f} Wh ¬∑ üåø {data['co2_g']:.4f} g ¬∑ üí∞ ${data['cost_usd']:.5f} ¬∑ ‚è±Ô∏è {data['latency_s']:.1f}s
            </div>
        </div>
    </div>
    """, unsafe_allow_html=True)


def render_extraction_results(results: Dict):
    """Affiche les r√©sultats d√©taill√©s de l'extraction c√¥te √† c√¥te."""
    st.markdown("---")
    st.subheader("R√©sultats du Benchmark")

    render_winner_badge(results)
    render_comparison_chart(results)

    st.markdown("### Analyse par mod√®le")

    valid_results = sort_results_by_model_size({k: v for k, v in results.items() if v.get("success")})
    cols = st.columns(len(valid_results))

    for col_idx, (_, data) in enumerate(valid_results.items()):
        with cols[col_idx]:
            with st.container(border=True):
                render_compact_metrics(data)

                st.markdown("**Donn√©es extraites :**")
                raw = data.get("raw_response")
                if raw:
                    try:
                        if isinstance(raw, str):
                            parsed = json.loads(raw)
                        else:
                            parsed = raw
                        st.json(parsed)
                    except (json.JSONDecodeError, TypeError):
                        st.code(str(raw), language="json")
                else:
                    st.info("Aucune donn√©e")


def render_agent_results(results: Dict):
    """Affiche les r√©sultats d√©taill√©s de l'agent c√¥te √† c√¥te."""
    st.markdown("---")
    st.subheader("R√©sultats du Benchmark")

    render_winner_badge(results)
    render_comparison_chart(results)

    st.markdown("### Analyse par mod√®le")

    valid_results = sort_results_by_model_size({k: v for k, v in results.items() if v.get("success")})
    cols = st.columns(len(valid_results))

    for col_idx, (_, data) in enumerate(valid_results.items()):
        with cols[col_idx]:
            with st.container(border=True):
                render_compact_metrics(data)

                raw = data.get("raw_response")
                if raw:
                    try:
                        if isinstance(raw, str):
                            parsed = json.loads(raw)
                        else:
                            parsed = raw

                        # Niveau de criticit√©
                        criticity = parsed.get("criticity", parsed.get("gravity_level", ""))
                        if criticity:
                            color = TRIAGE_COLORS.get(criticity.upper(), "#6B7280")
                            st.markdown(f"""
                                <div style="
                                    background: {color}; color: white;
                                    padding: 10px; border-radius: 8px;
                                    text-align: center; font-weight: bold;
                                    margin: 10px 0;
                                ">TRIAGE : {criticity.upper()}</div>
                            """, unsafe_allow_html=True)

                        # Justification
                        if parsed.get("justification"):
                            st.markdown(f"**Justification :** {parsed['justification']}")

                        # Bloc uniforme pour Infos manquantes
                        missing = parsed.get("missing_info", [])
                        if missing:
                            missing_list = "".join([f"<li>{m}</li>" for m in missing])
                            st.markdown(f"""
                                <div style="
                                    background: #FEF3C7; border-left: 4px solid #F59E0B;
                                    padding: 12px; border-radius: 4px; margin: 10px 0;
                                ">
                                    <strong style="color: #B45309;">‚ö†Ô∏è Informations manquantes :</strong>
                                    <ul style="margin: 5px 0 0 15px; padding: 0;">{missing_list}</ul>
                                </div>
                            """, unsafe_allow_html=True)

                        # Bloc uniforme pour Alerte protocole
                        if parsed.get("protocol_alert"):
                            st.markdown(f"""
                                <div style="
                                    background: #FEE2E2; border-left: 4px solid #DC2626;
                                    padding: 12px; border-radius: 4px; margin: 10px 0;
                                ">
                                    <strong style="color: #DC2626;">üö® Alerte protocole :</strong><br>
                                    {parsed['protocol_alert']}
                                </div>
                            """, unsafe_allow_html=True)

                    except (json.JSONDecodeError, TypeError):
                        st.code(str(raw))
                else:
                    st.info("Aucune analyse")


def render_simulation_results(results: Dict, nurse_message: str):
    """Affiche les r√©sultats d√©taill√©s de la simulation c√¥te √† c√¥te."""
    st.markdown("---")
    st.subheader("R√©sultats du Benchmark")

    render_winner_badge(results)
    render_comparison_chart(results)

    st.markdown("### R√©ponses des patients simul√©s")
    st.info(f"**Question de l'infirmier :** {nurse_message}")

    valid_results = sort_results_by_model_size({k: v for k, v in results.items() if v.get("success")})
    cols = st.columns(len(valid_results))

    for col_idx, (_, data) in enumerate(valid_results.items()):
        with cols[col_idx]:
            with st.container(border=True):
                render_compact_metrics(data)

                response = data.get("raw_response", "")
                if response:
                    st.markdown(f"""
                        <div style="
                            background: #F3F4F6; padding: 15px;
                            border-radius: 8px; font-style: italic;
                            margin: 10px 0;
                        ">"{response}"</div>
                    """, unsafe_allow_html=True)
                else:
                    st.caption("Pas de r√©ponse")


def main():
    """Point d'entr√©e principal."""
    init_benchmark_state()

    st.title("üå± Benchmark √âco-Performance")
    st.caption("Comparez les mod√®les Mistral et trouvez le plus sobre √©nerg√©tiquement")

    with st.container(border=True):
        st.markdown("""
        **Objectif :** Comparer les mod√®les Mistral sur 3 cas d'usage
        pour identifier le meilleur √©quilibre entre performance et sobri√©t√© √©nerg√©tique.

        S√©lectionnez les mod√®les √† comparer puis lancez le benchmark.
        """)

    # Bouton R√©initialiser en haut √† droite
    col_spacer, col_reset = st.columns([5, 1])
    with col_reset:
        if st.button("üîÑ R√©initialiser", type="secondary", use_container_width=True):
            reset_benchmark_state()
            st.rerun()

    tab1, tab2, tab3 = st.tabs(["üìã Extraction", "ü§ñ Agent Triage", "üí¨ Simulation"])

    # === ONGLET EXTRACTION ===
    with tab1:
        st.markdown("### Extraction de donn√©es m√©dicales")
        st.caption("Compare l'extraction d'informations structur√©es depuis une conversation")

        conversations = load_conversations()

        if not conversations:
            st.error("Impossible de charger les conversations. V√©rifiez la connexion au backend.")
        else:
            conv_options = {c["filename"]: f"{c['name']} ({c['niveau']})" for c in conversations}
            selected_conv = st.selectbox(
                "Conversation de test",
                options=list(conv_options.keys()),
                format_func=lambda x: conv_options[x],
                key="extraction_conv"
            )

            st.markdown("")
            selected_models = render_model_selector("extraction")

            st.markdown("")
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                btn_disabled = len(selected_models) < 2
                if st.button(
                    "üöÄ Lancer le benchmark",
                    type="primary",
                    disabled=btn_disabled,
                    use_container_width=True,
                    key="run_extraction"
                ):
                    conv_data = load_conversation_content(selected_conv)
                    if conv_data:
                        full_text = "\n".join([
                            f"{'Infirmier' if m['role'] == 'infirmier' else 'Patient'} : {m['content']}"
                            for m in conv_data.get("messages", [])
                        ])
                        progress = st.empty()
                        results = run_extraction_benchmark(selected_models, full_text, progress)
                        progress.empty()
                        st.session_state.benchmark_results["extraction"] = results
                        st.rerun()

            if btn_disabled:
                st.info("S√©lectionnez au moins 2 mod√®les pour comparer")

            if "extraction" in st.session_state.benchmark_results:
                render_extraction_results(st.session_state.benchmark_results["extraction"])

    # === ONGLET AGENT ===
    with tab2:
        st.markdown("### Agent de Triage")
        st.caption("Compare l'analyse et la classification de criticit√©")

        conversations = load_conversations()

        if not conversations:
            st.error("Impossible de charger les conversations.")
        else:
            conv_options = {c["filename"]: f"{c['name']} ({c['niveau']})" for c in conversations}
            selected_conv = st.selectbox(
                "Conversation de test",
                options=list(conv_options.keys()),
                format_func=lambda x: conv_options[x],
                key="agent_conv"
            )

            st.markdown("")
            selected_models = render_model_selector("agent")

            st.markdown("")
            col1, col2, col3 = st.columns([1, 2, 1])
            with col2:
                btn_disabled = len(selected_models) < 2
                if st.button(
                    "üöÄ Lancer le benchmark",
                    type="primary",
                    disabled=btn_disabled,
                    use_container_width=True,
                    key="run_agent"
                ):
                    conv_data = load_conversation_content(selected_conv)
                    if conv_data:
                        progress = st.empty()
                        results = run_agent_benchmark(selected_models, conv_data, progress)
                        progress.empty()
                        st.session_state.benchmark_results["agent"] = results
                        st.rerun()

            if btn_disabled:
                st.info("S√©lectionnez au moins 2 mod√®les pour comparer")

            if "agent" in st.session_state.benchmark_results:
                render_agent_results(st.session_state.benchmark_results["agent"])

    # === ONGLET SIMULATION ===
    with tab3:
        st.markdown("### Simulation Patient")
        st.caption("Compare la g√©n√©ration de r√©ponses patient r√©alistes")

        sim_options = {k: v["name"] for k, v in SIMULATION_CASES.items()}
        selected_sim = st.selectbox(
            "Cas de simulation",
            options=list(sim_options.keys()),
            format_func=lambda x: sim_options[x],
            key="simulation_case"
        )

        sim_case = SIMULATION_CASES[selected_sim]

        with st.expander("Voir le profil du patient"):
            st.markdown(f"```\n{sim_case['persona']}\n```")

        # Choix de la question de l'infirmier
        selected_question = st.selectbox(
            "Question de l'infirmier",
            options=sim_case["questions"],
            key="nurse_question"
        )

        st.markdown("")
        selected_models = render_model_selector("simulation")

        st.markdown("")
        col1, col2, col3 = st.columns([1, 2, 1])
        with col2:
            btn_disabled = len(selected_models) < 2
            if st.button(
                "üöÄ Lancer le benchmark",
                type="primary",
                disabled=btn_disabled,
                use_container_width=True,
                key="run_simulation"
            ):
                progress = st.empty()
                results = run_simulation_benchmark(selected_models, sim_case, selected_question, progress)
                progress.empty()
                st.session_state.benchmark_results["simulation"] = results
                st.session_state.benchmark_results["simulation_question"] = selected_question
                st.rerun()

        if btn_disabled:
            st.info("S√©lectionnez au moins 2 mod√®les pour comparer")

        if "simulation" in st.session_state.benchmark_results:
            render_simulation_results(
                st.session_state.benchmark_results["simulation"],
                st.session_state.benchmark_results.get("simulation_question", selected_question)
            )

    # Footer avec justification Mistral
    st.markdown("---")
    with st.expander("‚ÑπÔ∏è Pourquoi uniquement des mod√®les Mistral ?"):
        st.markdown("""
        **Choix de Mistral AI pour ce benchmark :**

        1. **Comparaison √©quitable** : En utilisant des mod√®les du m√™me fournisseur, on √©limine les biais
           li√©s aux diff√©rences d'infrastructure, de tokenisation et de mesure.

        2. **Support EcoLogits** : La biblioth√®que EcoLogits que nous utilisons pour les mesures
           environnementales est optimis√©e pour les mod√®les Mistral.

        3. **Souverainet√© europ√©enne** : Mistral AI est une entreprise fran√ßaise, ce qui s'aligne
           avec les enjeux de souverainet√© num√©rique en sant√©.

        4. **Rapport qualit√©/prix** : Les mod√®les Mistral offrent un excellent compromis entre
           performance et co√ªt, adapt√© √† un contexte hospitalier.

        5. **Gamme compl√®te** : De Ministral 3B (ultra-l√©ger) √† Mistral Large (haute pr√©cision),
           la gamme permet de tester diff√©rents niveaux de compromis.

        *Pour √©tendre √† d'autres fournisseurs (OpenAI, Anthropic), il faudrait standardiser
        les m√©triques environnementales, ce qui reste un d√©fi technique.*
        """)


if __name__ == "__main__":
    main()
